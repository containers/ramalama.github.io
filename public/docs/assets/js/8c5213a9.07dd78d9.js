"use strict";(globalThis.webpackChunkdocsite=globalThis.webpackChunkdocsite||[]).push([[235],{7119:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"configuration/conf","title":"Configuration File","description":"Configuration file reference","source":"@site/docs/configuration/conf.mdx","sourceDirName":"configuration","slug":"/configuration/conf","permalink":"/docs/configuration/conf","draft":false,"unlisted":false,"editUrl":"https://github.com/containers/ramalama/edit/main/docsite/configuration/conf.mdx","tags":[],"version":"current","frontMatter":{"title":"Configuration File","description":"Configuration file reference"},"sidebar":"docs","previous":{"title":"Configuration","permalink":"/docs/category/configuration"},"next":{"title":"OCI Spec","permalink":"/docs/configuration/ramalama-oci"}}');var a=r(4848),s=r(8453);const t={title:"Configuration File",description:"Configuration file reference"},l="Configuration File",o={},d=[{value:"ENVIRONMENT VARIABLES",id:"environment-variables",level:2},{value:"RAMALAMA TABLE",id:"ramalama-table",level:2},{value:"RAMALAMA.USER TABLE",id:"ramalamauser-table",level:2}];function c(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"configuration-file",children:"Configuration File"})}),"\n",(0,a.jsx)(n.h1,{id:"description",children:"DESCRIPTION"}),"\n",(0,a.jsx)(n.p,{children:"RamaLama reads all ramalama.conf files, if they exists\nand modify the defaults for running RamaLama on the host. ramalama.conf uses\na TOML format that can be easily modified and versioned."}),"\n",(0,a.jsx)(n.p,{children:"RamaLama reads the he following paths for global configuration that effects all users."}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Paths"}),(0,a.jsx)(n.th,{children:"Exception"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"/usr/share/ramalama/ramalama.conf"})}),(0,a.jsx)(n.td,{children:"On Linux"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"/usr/local/share/ramalama/ramalama.conf"})}),(0,a.jsx)(n.td,{children:"On Linux"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"/etc/ramalama/ramalama.conf"})}),(0,a.jsx)(n.td,{children:"On Linux"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"/etc/ramalama/ramalama.conf.d/*.conf"})}),(0,a.jsx)(n.td,{children:"On Linux"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"$HOME/.local/.pipx/venvs/usr/share/ramalama/ramalama.conf"})}),(0,a.jsx)(n.td,{children:"On pipx installed macOS"})]})]})]}),"\n",(0,a.jsx)(n.p,{children:"For user specific configuration it reads"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Paths"}),(0,a.jsx)(n.th,{children:"Exception"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"$XDG_CONFIG_HOME/ramalama/ramalama.conf"})}),(0,a.jsx)(n.td,{})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"$XDG_CONFIG_HOME/ramalama/ramalama.conf.d/*.conf"})}),(0,a.jsx)(n.td,{})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"$HOME/.config/ramalama/ramalama.conf"})}),(0,a.jsxs)(n.td,{children:[(0,a.jsx)(n.code,{children:"$XDG_CONFIG_HOME"})," not set"]})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"$HOME/.config/ramalama/ramalama.conf.d/*.conf"})}),(0,a.jsxs)(n.td,{children:[(0,a.jsx)(n.code,{children:"$XDG_CONFIG_HOME"})," not set"]})]})]})]}),"\n",(0,a.jsx)(n.p,{children:"Fields specified in ramalama conf files override the default options, as well as\noptions in previously read ramalama conf files."}),"\n",(0,a.jsxs)(n.p,{children:["Config files in the ",(0,a.jsx)(n.code,{children:".d"})," directories, are added in alpha numeric sorted order and must end in ",(0,a.jsx)(n.code,{children:".conf"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"environment-variables",children:"ENVIRONMENT VARIABLES"}),"\n",(0,a.jsxs)(n.p,{children:["If the ",(0,a.jsx)(n.code,{children:"RAMALAMA_CONFIG"})," environment variable is set, all system and user\nconfig files are ignored and only the specified config file is loaded."]}),"\n",(0,a.jsx)(n.h1,{id:"format",children:"FORMAT"}),"\n",(0,a.jsx)(n.p,{children:"The [TOML format][toml] is used as the encoding of the configuration file.\nEvery option is nested under its table. No bare options are used. The format of\nTOML can be simplified to:"}),"\n",(0,a.jsx)(n.p,{children:"[table1]\noption = value"}),"\n",(0,a.jsx)(n.p,{children:"[table2]\noption = value"}),"\n",(0,a.jsx)(n.p,{children:"[table3]\noption = value"}),"\n",(0,a.jsx)(n.p,{children:"[table3.subtable1]\noption = value"}),"\n",(0,a.jsx)(n.h2,{id:"ramalama-table",children:"RAMALAMA TABLE"}),"\n",(0,a.jsx)(n.p,{children:"The ramalama table contains settings to configure and manage the OCI runtime."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"[[ramalama]]"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"api"}),'="none"']}),"\n",(0,a.jsx)(n.p,{children:"Unified API layer for Inference, RAG, Agents, Tools, Safety, Evals, and Telemetry.\nOptions: llama-stack, none"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"api_key"}),'=""']}),"\n",(0,a.jsx)(n.p,{children:"OpenAI-compatible API key. Can also be set via the RAMALAMA_API_KEY environment variable."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"carimage"}),'="registry.access.redhat.com/ubi10-micro',":latest",'"']}),"\n",(0,a.jsx)(n.p,{children:"OCI model car image"}),"\n",(0,a.jsx)(n.p,{children:"Image to be used when building and pushing --type=car models"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"cache_reuse"}),"=256"]}),"\n",(0,a.jsx)(n.p,{children:"Min chunk size to attempt reusing from the cache via KV shifting"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"container"}),"=true"]}),"\n",(0,a.jsx)(n.p,{children:"Run RamaLama in the default container.\nRAMALAMA_IN_CONTAINER environment variable overrides this field."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"ctx_size"}),"=0"]}),"\n",(0,a.jsx)(n.p,{children:"Size of the prompt context (0 = loaded from model)"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"engine"}),'="podman"']}),"\n",(0,a.jsx)(n.p,{children:"Run RamaLama using the specified container engine.\nValid options are: Podman and Docker\nThis field can be overridden by the RAMALAMA_CONTAINER_ENGINE environment variable."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"env"}),"=[]"]}),"\n",(0,a.jsx)(n.p,{children:'Environment variables to be added to the environment used when running in a container engine (e.g., Podman, Docker). For example "LLAMA_ARG_THREADS=10".'}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"gguf_quantization_mode"}),'="Q4_K_M"']}),"\n",(0,a.jsx)(n.p,{children:"The quantization mode used when creating OCI formatted AI Models.\nAvailable options: Q2_K, Q3_K_S, Q3_K_M, Q3_K_L, Q4_0, Q4_K_S, Q4_K_M, Q5_0, Q5_K_S, Q5_K_M, Q6_K, Q8_0."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"host"}),'="0.0.0.0"']}),"\n",(0,a.jsx)(n.p,{children:"IP address for llama.cpp to listen on."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"image"}),'="quay.io/ramalama/ramalama',":latest",'"']}),"\n",(0,a.jsx)(n.p,{children:"OCI container image to run with the specified AI model\nRAMALAMA_IMAGE environment variable overrides this field."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"[[ramalama.images]]"}),'\nHIP_VISIBLE_DEVICES    = "quay.io/ramalama/rocm"\nCUDA_VISIBLE_DEVICES   = "quay.io/ramalama/cuda"\nASAHI_VISIBLE_DEVICES  = "quay.io/ramalama/asahi"\nINTEL_VISIBLE_DEVICES  = "quay.io/ramalama/intel-gpu"\nASCEND_VISIBLE_DEVICES = "quay.io/ramalama/cann"\nMUSA_VISIBLE_DEVICES   = "quay.io/ramalama/musa"\nVLLM                   = "registry.redhat.io/rhelai1/ramalama-vllm"']}),"\n",(0,a.jsx)(n.p,{children:"Alternative images to use when RamaLama recognizes specific hardware or user\nspecified vllm model runtime."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"keep_groups"}),"=false"]}),"\n",(0,a.jsxs)(n.p,{children:["Pass ",(0,a.jsx)(n.code,{children:"--group-add keep-groups"})," to podman, when using podman.\nIn some cases this is needed to access the gpu from a rootless container"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"log_level"}),"=warning\nSet the logging level of RamaLama application.\nValid Values:\ndebug, info, warning, error critical"]}),"\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsx)(n.p,{children:"--debug option overrides this field and forces the system to debug"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"max_tokens"}),"=0"]}),"\n",(0,a.jsx)(n.p,{children:"Maximum number of tokens to generate. Set to 0 for unlimited output (default: 0).\nThis parameter is mapped to the appropriate runtime-specific parameter when executing models."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"ngl"}),"=-1"]}),"\n",(0,a.jsx)(n.p,{children:"number of gpu layers, 0 means CPU inferencing, 999 means use max layers (default: -1)\nThe default -1, means use whatever is automatically deemed appropriate (0 or 999)"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"prefix"}),'=""\nSpecify default prefix for chat and run command. By default the prefix\nis based on the container engine used.']}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Container Engine"}),(0,a.jsx)(n.th,{children:"Prefix"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Podman"}),(0,a.jsx)(n.td,{children:'"\ud83e\uddad > "'})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Docker"}),(0,a.jsx)(n.td,{children:'"\ud83d\udc0b > "'})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"No Engine"}),(0,a.jsx)(n.td,{children:'"\ud83e\udd99 > "'})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"No EMOJI support"}),(0,a.jsx)(n.td,{children:'"> "'})]})]})]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"port"}),'="8080"']}),"\n",(0,a.jsx)(n.p,{children:"Specify initial port for a range of 101 ports for services to listen on.\nIf this port is unavailable, another free port from this range will be selected."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"pull"}),'="newer"']}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"always"}),": Always pull the image and throw an error if the pull fails."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"missing"}),": Only pull the image when it does not exist in the local containers storage. Throw an error if no image is found and the pull fails."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"never"}),": Never pull the image but use the one from the local containers storage. Throw an error when no image is found."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"newer"}),": Pull if the image on the registry is newer than the one in the local containers storage. An image is considered to be newer when the digests are different. Comparing the time stamps is prone to errors. Pull errors are suppressed if a local image was found."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"rag_format"}),'="qdrant"']}),"\n",(0,a.jsxs)(n.p,{children:["Specify the default output format for output of the ",(0,a.jsx)(n.code,{children:"ramalama rag"})," command.\nOptions: qdrant, json, markdown, milvus."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"rag_images"}),'="quay.io/ramalama/ramalama-rag"']}),"\n",(0,a.jsx)(n.p,{children:"OCI container image to run with the specified AI model when using RAG content."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"[[ramalama.rag_images]]"}),'\nCUDA_VISIBLE_DEVICES   = "quay.io/ramalama/cuda-rag"\nHIP_VISIBLE_DEVICES    = "quay.io/ramalama/rocm-rag"\nINTEL_VISIBLE_DEVICES  = "quay.io/ramalama/intel-gpu-rag"\nGGML_VK_VISIBLE_DEVICES = "quay.io/ramalama/ramalama"']}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"runtime"}),'="llama.cpp"']}),"\n",(0,a.jsx)(n.p,{children:"Specify the AI runtime to use; valid options are 'llama.cpp', 'vllm', and 'mlx' (default: llama.cpp)\nOptions: llama.cpp, vllm, mlx"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"selinux"}),"=false"]}),"\n",(0,a.jsx)(n.p,{children:"SELinux container separation enforcement"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"store"}),'="$HOME/.local/share/ramalama"']}),"\n",(0,a.jsx)(n.p,{children:"Store AI Models in the specified directory"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"summarize_after"}),"=4"]}),"\n",(0,a.jsx)(n.p,{children:"Automatically summarize conversation history after N messages to prevent context growth.\nWhen enabled, ramalama will periodically condense older messages into a summary,\nkeeping only recent messages and the summary. This prevents the context from growing\nindefinitely during long chat sessions. Set to 0 to disable (default: 4)."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"temp"}),'="0.8"\nTemperature of the response from the AI Model\nllama.cpp explains this as:']}),"\n",(0,a.jsx)(n.p,{children:"The lower the number is, the more deterministic the response."}),"\n",(0,a.jsx)(n.p,{children:"The higher the number is the more creative the response is, but more likely to hallucinate when set too high."}),"\n",(0,a.jsx)(n.p,{children:"Usage: Lower numbers are good for virtual assistants where we need deterministic responses. Higher numbers are good for roleplay or creative tasks like editing stories"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thinking"}),"=true"]}),"\n",(0,a.jsx)(n.p,{children:"Enable thinking mode on reasoning models"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"threads"}),"=-1"]}),"\n",(0,a.jsx)(n.p,{children:"maximum number of cpu threads to use for inferencing\nThe default -1, uses the default of the underlying implementation"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"transport"}),'="ollama"']}),"\n",(0,a.jsx)(n.p,{children:"Specify the default transport to be used for pulling and pushing of AI Models.\nOptions: oci, ollama, huggingface.\nRAMALAMA_TRANSPORT environment variable overrides this field."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"[[ramalama.http_client]]"})}),"\n",(0,a.jsx)(n.p,{children:"Http client configuration"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"max_retries"}),"=5"]}),"\n",(0,a.jsx)(n.p,{children:"The maximum number of times to retry a failed download"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"max_retry_delay"}),"=30"]}),"\n",(0,a.jsx)(n.p,{children:"The maximum delay between retry attempts in seconds"}),"\n",(0,a.jsx)(n.h2,{id:"ramalamauser-table",children:"RAMALAMA.USER TABLE"}),"\n",(0,a.jsx)(n.p,{children:"The ramalama.user table contains user preference settings."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"[[ramalama.user]]"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"no_missing_gpu_prompt"}),"=false"]}),"\n",(0,a.jsx)(n.p,{children:"Suppress the interactive prompt when running on macOS with a Podman VM that does not support GPU acceleration (e.g., applehv provider). When set to true, RamaLama will automatically proceed without GPU support instead of prompting the user for confirmation. This is useful for automation and scripting scenarios where interactive prompts are not desired."}),"\n",(0,a.jsx)(n.p,{children:"Can also be set via the RAMALAMA_USER__NO_MISSING_GPU_PROMPT environment variable."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>l});var i=r(6540);const a={},s=i.createContext(a);function t(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);