"use strict";(globalThis.webpackChunkdocsite=globalThis.webpackChunkdocsite||[]).push([[836],{4094:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"commands/ramalama/ramalama","title":"ramalama","description":"Simple management tool for working with AI Models","source":"@site/docs/commands/ramalama/ramalama.mdx","sourceDirName":"commands/ramalama","slug":"/commands/ramalama/","permalink":"/docs/commands/ramalama/","draft":false,"unlisted":false,"editUrl":"https://github.com/containers/ramalama/edit/main/docsite/commands/ramalama/ramalama.mdx","tags":[],"version":"current","frontMatter":{"title":"ramalama","description":"Simple management tool for working with AI Models"},"sidebar":"docs","previous":{"title":"Commands","permalink":"/docs/category/commands"},"next":{"title":"bench","permalink":"/docs/commands/ramalama/bench"}}');var s=n(4848),i=n(8453);const t={title:"ramalama",description:"Simple management tool for working with AI Models"},l="ramalama",o={},d=[{value:"Synopsis",id:"synopsis",level:2},{value:"Description",id:"description",level:2},{value:"SECURITY",id:"security",level:2},{value:"Test and run your models more securely",id:"test-and-run-your-models-more-securely",level:3},{value:"Here\u2019s how RamaLama delivers a robust security footprint:",id:"heres-how-ramalama-delivers-a-robust-security-footprint",level:3},{value:"MODEL TRANSPORTS",id:"model-transports",level:2},{value:"GLOBAL OPTIONS",id:"global-options",level:2},{value:"<strong>--debug</strong>",id:"--debug",level:4},{value:"<strong>--dryrun</strong>",id:"--dryrun",level:4},{value:"<strong>--engine</strong>",id:"--engine",level:4},{value:"<strong>--help</strong>, <strong>-h</strong>",id:"--help--h",level:4},{value:"<strong>--nocontainer</strong>",id:"--nocontainer",level:4},{value:"<strong>--quiet</strong>",id:"--quiet",level:4},{value:"<strong>--runtime</strong>=<em>llama.cpp</em> | <em>vllm</em>",id:"--runtimellamacpp--vllm",level:4},{value:"<strong>--store</strong>=STORE",id:"--storestore",level:4},{value:"COMMANDS",id:"commands",level:2},{value:"CONFIGURATION FILES",id:"configuration-files",level:2},{value:"ENVIRONMENT VARIABLES",id:"environment-variables",level:2},{value:"See Also",id:"see-also",level:2}];function c(e){const a={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.header,{children:(0,s.jsx)(a.h1,{id:"ramalama",children:"ramalama"})}),"\n",(0,s.jsx)(a.h2,{id:"synopsis",children:"Synopsis"}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"ramalama"})," [",(0,s.jsx)(a.em,{children:"options"}),"] ",(0,s.jsx)(a.em,{children:"command"})]}),"\n",(0,s.jsx)(a.h2,{id:"description",children:"Description"}),"\n",(0,s.jsx)(a.p,{children:"RamaLama : The goal of RamaLama is to make AI boring."}),"\n",(0,s.jsx)(a.p,{children:"RamaLama tool facilitates local management and serving of AI Models."}),"\n",(0,s.jsx)(a.p,{children:"On first run RamaLama inspects your system for GPU support, falling back to CPU support if no GPUs are present."}),"\n",(0,s.jsx)(a.p,{children:"RamaLama uses container engines like Podman or Docker to pull the appropriate OCI image with all of the software necessary to run an AI Model for your systems setup."}),"\n",(0,s.jsxs)(a.p,{children:["Running in containers eliminates the need for users to configure the host\nsystem for AI. After the initialization, RamaLama runs the AI Models within a\ncontainer based on the OCI image. RamaLama pulls container image specific to\nthe GPUs discovered on the host system. These images are tied to the minor\nversion of RamaLama. For example RamaLama version 1.2.3 on an NVIDIA system\npulls quay.io/ramalama/cuda:1.2. To override the default image use the\n",(0,s.jsx)(a.code,{children:"--image"})," option."]}),"\n",(0,s.jsx)(a.p,{children:"RamaLama pulls AI Models from model registries. Starting a chatbot or a rest API service from a simple single command. Models are treated similarly to how Podman and Docker treat container images."}),"\n",(0,s.jsxs)(a.p,{children:["When both Podman and Docker are installed, RamaLama defaults to Podman, The ",(0,s.jsx)(a.code,{children:"RAMALAMA_CONTAINER_ENGINE=docker"})," environment variable can override this behaviour. When neither are installed RamaLama attempts to run the model with software on the local system."]}),"\n",(0,s.jsx)(a.admonition,{type:"note",children:(0,s.jsxs)(a.p,{children:["On MacOS systems that use Podman for containers, configure the Podman machine to use the ",(0,s.jsx)(a.code,{children:"libkrun"})," machine provider. The ",(0,s.jsx)(a.code,{children:"libkrun"})," provider enables containers within the Podman Machine access to the Mac's GPU. See ",(0,s.jsx)(a.a,{href:"/docs/platform-guides/macos",children:"ramalama-macos(7)"})," for further information."]})}),"\n",(0,s.jsx)(a.admonition,{type:"note",children:(0,s.jsxs)(a.p,{children:["On systems with NVIDIA GPUs, see ",(0,s.jsx)(a.a,{href:"/docs/platform-guides/cuda",children:"ramalama-cuda(7)"})," to correctly configure the host system."]})}),"\n",(0,s.jsxs)(a.p,{children:["RamaLama CLI defaults can be modified via ramalama.conf files. Default settings for flags are defined in ",(0,s.jsx)(a.a,{href:"/docs/configuration/conf",children:"ramalama.conf(5)"}),"."]}),"\n",(0,s.jsx)(a.h2,{id:"security",children:"SECURITY"}),"\n",(0,s.jsx)(a.h3,{id:"test-and-run-your-models-more-securely",children:"Test and run your models more securely"}),"\n",(0,s.jsxs)(a.p,{children:["Because RamaLama defaults to running AI models inside of rootless containers using Podman on Docker. These containers isolate the AI models from information on the underlying host. With RamaLama containers, the AI model is mounted as a volume into the container in read/only mode. This results in the process running the model, llama.cpp or vLLM, being isolated from the host.  In addition, since ",(0,s.jsx)(a.code,{children:"ramalama run"})," uses the --network=none option, the container can not reach the network and leak any information out of the system. Finally, containers are run with --rm options which means that any content written during the running of the container is wiped out when the application exits."]}),"\n",(0,s.jsx)(a.h3,{id:"heres-how-ramalama-delivers-a-robust-security-footprint",children:"Here\u2019s how RamaLama delivers a robust security footprint:"}),"\n",(0,s.jsx)(a.p,{children:"\u2705 Container Isolation \u2013 AI models run within isolated containers, preventing direct access to the host system.\n\u2705 Read-Only Volume Mounts \u2013 The AI model is mounted in read-only mode, meaning that processes inside the container cannot modify host files.\n\u2705 No Network Access \u2013 ramalama run is executed with --network=none, meaning the model has no outbound connectivity for which information can be leaked.\n\u2705 Auto-Cleanup \u2013 Containers run with --rm, wiping out any temporary data once the session ends.\n\u2705 Drop All Linux Capabilities \u2013 No access to Linux capabilities to attack the underlying host.\n\u2705 No New Privileges \u2013 Linux Kernel feature which disables container processes from gaining additional privileges."}),"\n",(0,s.jsx)(a.h2,{id:"model-transports",children:"MODEL TRANSPORTS"}),"\n",(0,s.jsx)(a.p,{children:"RamaLama supports multiple AI model registries types called transports. Supported transports:"}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"Transports"}),(0,s.jsx)(a.th,{children:"Prefix"}),(0,s.jsx)(a.th,{children:"Web Site"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"URL based"}),(0,s.jsx)(a.td,{children:"https://, http://, file://"}),(0,s.jsxs)(a.td,{children:[(0,s.jsx)(a.code,{children:"https://web.site/ai.model"}),", ",(0,s.jsx)(a.code,{children:"file://tmp/ai.model"})]})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"HuggingFace"}),(0,s.jsx)(a.td,{children:"huggingface://, hf://, hf.co/"}),(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"https://www.huggingface.co",children:(0,s.jsx)(a.code,{children:"huggingface.co"})})})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"ModelScope"}),(0,s.jsx)(a.td,{children:"modelscope://, ms://"}),(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"https://modelscope.cn/",children:(0,s.jsx)(a.code,{children:"modelscope.cn"})})})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Ollama"}),(0,s.jsx)(a.td,{children:"ollama://"}),(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"https://www.ollama.com",children:(0,s.jsx)(a.code,{children:"ollama.com"})})})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"rlcr"}),(0,s.jsx)(a.td,{children:"rlcr://"}),(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"https://registry.ramalama.com",children:(0,s.jsx)(a.code,{children:"ramalama.com"})})})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"OCI Container Registries"}),(0,s.jsx)(a.td,{children:"oci://"}),(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"https://opencontainers.org",children:(0,s.jsx)(a.code,{children:"opencontainers.org"})})})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{}),(0,s.jsx)(a.td,{}),(0,s.jsxs)(a.td,{children:["Examples: ",(0,s.jsx)(a.a,{href:"https://quay.io",children:(0,s.jsx)(a.code,{children:"quay.io"})}),",  ",(0,s.jsx)(a.a,{href:"https://docker.io",children:(0,s.jsx)(a.code,{children:"Docker Hub"})}),",",(0,s.jsx)(a.a,{href:"https://artifactory.com",children:(0,s.jsx)(a.code,{children:"Artifactory"})})]})]})]})]}),"\n",(0,s.jsxs)(a.p,{children:["RamaLama uses to the Ollama registry transport. This default can be overridden in the ",(0,s.jsx)(a.code,{children:"ramalama.conf"})," file or via the RAMALAMA_TRANSPORTS\nenvironment. ",(0,s.jsx)(a.code,{children:"export RAMALAMA_TRANSPORT=huggingface"})," Changes RamaLama to use huggingface transport."]}),"\n",(0,s.jsxs)(a.p,{children:["Modify individual model transports by specifying the ",(0,s.jsx)(a.code,{children:"huggingface://"}),", ",(0,s.jsx)(a.code,{children:"oci://"}),", ",(0,s.jsx)(a.code,{children:"ollama://"}),", ",(0,s.jsx)(a.code,{children:"https://"}),", ",(0,s.jsx)(a.code,{children:"http://"}),", ",(0,s.jsx)(a.code,{children:"file://"})," prefix to the model."]}),"\n",(0,s.jsx)(a.p,{children:"URL support means if a model is on a web site or even on your local system, you can run it directly."}),"\n",(0,s.jsxs)(a.p,{children:["ramalama pull ",(0,s.jsx)(a.code,{children:"huggingface://"}),"afrideva/Tiny-Vicuna-1B-GGUF/tiny-vicuna-1b.q2_k.gguf"]}),"\n",(0,s.jsxs)(a.p,{children:["ramalama run ",(0,s.jsx)(a.code,{children:"file://"}),"$HOME/granite-7b-lab-Q4_K_M.gguf"]}),"\n",(0,s.jsx)(a.p,{children:"To make it easier for users, RamaLama uses shortname files, which container\nalias names for fully specified AI Models allowing users to specify the shorter\nnames when referring to models. RamaLama reads shortnames.conf files if they\nexist . These files contain a list of name value pairs for specification of\nthe model. The following table specifies the order which RamaLama reads the files\n. Any duplicate names that exist override previously defined shortnames."}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"Shortnames type"}),(0,s.jsx)(a.th,{children:"Path"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Distribution"}),(0,s.jsx)(a.td,{children:"/usr/share/ramalama/shortnames.conf"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Local install"}),(0,s.jsx)(a.td,{children:"/usr/local/share/ramalama/shortnames.conf"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Administrators"}),(0,s.jsx)(a.td,{children:"/etc/ramamala/shortnames.conf"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Users"}),(0,s.jsx)(a.td,{children:"$HOME/.config/ramalama/shortnames.conf"})]})]})]}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-toml",children:'$ cat /usr/share/ramalama/shortnames.conf\n[shortnames]\n  "tiny" = "ollama://tinyllama"\n  "granite" = "huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf"\n  "granite:7b" = "huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf"\n  "ibm/granite" = "huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf"\n  "merlinite" = "huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf"\n  "merlinite:7b" = "huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf"\n...\n'})}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.strong,{children:"ramalama [GLOBAL OPTIONS]"})}),"\n",(0,s.jsx)(a.h2,{id:"global-options",children:"GLOBAL OPTIONS"}),"\n",(0,s.jsx)(a.h4,{id:"--debug",children:(0,s.jsx)(a.strong,{children:"--debug"})}),"\n",(0,s.jsx)(a.p,{children:"print debug messages"}),"\n",(0,s.jsx)(a.h4,{id:"--dryrun",children:(0,s.jsx)(a.strong,{children:"--dryrun"})}),"\n",(0,s.jsx)(a.p,{children:"show container runtime command without executing it (default: False)"}),"\n",(0,s.jsx)(a.h4,{id:"--engine",children:(0,s.jsx)(a.strong,{children:"--engine"})}),"\n",(0,s.jsxs)(a.p,{children:["run RamaLama using the specified container engine. Default is ",(0,s.jsx)(a.code,{children:"podman"})," if installed otherwise docker.\nThe default can be overridden in the ramalama.conf file or via the RAMALAMA_CONTAINER_ENGINE environment variable."]}),"\n",(0,s.jsxs)(a.h4,{id:"--help--h",children:[(0,s.jsx)(a.strong,{children:"--help"}),", ",(0,s.jsx)(a.strong,{children:"-h"})]}),"\n",(0,s.jsx)(a.p,{children:"show this help message and exit"}),"\n",(0,s.jsx)(a.h4,{id:"--nocontainer",children:(0,s.jsx)(a.strong,{children:"--nocontainer"})}),"\n",(0,s.jsx)(a.p,{children:"Do not run RamaLama workloads in containers (default: False)\nThe default can be overridden in the ramalama.conf file."}),"\n",(0,s.jsx)(a.admonition,{type:"note",children:(0,s.jsx)(a.p,{children:"OCI images cannot be used with the --nocontainer option. This option disables the following features: Automatic GPU acceleration, containerized environment isolation, and dynamic resource allocation. For a complete list of affected features, please see the RamaLama documentation at [link-to-feature-list]."})}),"\n",(0,s.jsx)(a.h4,{id:"--quiet",children:(0,s.jsx)(a.strong,{children:"--quiet"})}),"\n",(0,s.jsx)(a.p,{children:"Decrease output verbosity."}),"\n",(0,s.jsxs)(a.h4,{id:"--runtimellamacpp--vllm",children:[(0,s.jsx)(a.strong,{children:"--runtime"}),"=",(0,s.jsx)(a.em,{children:"llama.cpp"})," | ",(0,s.jsx)(a.em,{children:"vllm"})]}),"\n",(0,s.jsx)(a.p,{children:"specify the runtime to use, valid options are 'llama.cpp' and 'vllm' (default: llama.cpp)\nThe default can be overridden in the ramalama.conf file."}),"\n",(0,s.jsxs)(a.h4,{id:"--storestore",children:[(0,s.jsx)(a.strong,{children:"--store"}),"=STORE"]}),"\n",(0,s.jsxs)(a.p,{children:["store AI Models in the specified directory (default rootless: ",(0,s.jsx)(a.code,{children:"$HOME/.local/share/ramalama"}),", default rootful: ",(0,s.jsx)(a.code,{children:"/var/lib/ramalama"}),")\nThe default can be overridden in the ramalama.conf file."]}),"\n",(0,s.jsx)(a.h2,{id:"commands",children:"COMMANDS"}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"Command"}),(0,s.jsx)(a.th,{children:"Description"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/bench",children:"ramalama-bench(1)"})}),(0,s.jsx)(a.td,{children:"benchmark specified AI Model"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/chat",children:"ramalama-chat(1)"})}),(0,s.jsx)(a.td,{children:"OpenAI chat with the specified REST API URL"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/containers",children:"ramalama-containers(1)"})}),(0,s.jsx)(a.td,{children:"list all RamaLama containers"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/convert",children:"ramalama-convert(1)"})}),(0,s.jsx)(a.td,{children:"convert AI Models from local storage to OCI Image"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/daemon",children:"ramalama-daemon(1)"})}),(0,s.jsx)(a.td,{children:"run a RamaLama REST server"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/info",children:"ramalama-info(1)"})}),(0,s.jsx)(a.td,{children:"display RamaLama configuration information"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/inspect",children:"ramalama-inspect(1)"})}),(0,s.jsx)(a.td,{children:"inspect the specified AI Model"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/list",children:"ramalama-list(1)"})}),(0,s.jsx)(a.td,{children:"list all downloaded AI Models"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/login",children:"ramalama-login(1)"})}),(0,s.jsx)(a.td,{children:"login to remote registry"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/logout",children:"ramalama-logout(1)"})}),(0,s.jsx)(a.td,{children:"logout from remote registry"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/perplexity",children:"ramalama-perplexity(1)"})}),(0,s.jsx)(a.td,{children:"calculate the perplexity value of an AI Model"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/pull",children:"ramalama-pull(1)"})}),(0,s.jsx)(a.td,{children:"pull AI Models from Model registries to local storage"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/push",children:"ramalama-push(1)"})}),(0,s.jsx)(a.td,{children:"push AI Models from local storage to remote registries"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/rag",children:"ramalama-rag(1)"})}),(0,s.jsx)(a.td,{children:"generate and convert Retrieval Augmented Generation (RAG) data from provided documents into an OCI Image"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/rm",children:"ramalama-rm(1)"})}),(0,s.jsx)(a.td,{children:"remove AI Models from local storage"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/run",children:"ramalama-run(1)"})}),(0,s.jsx)(a.td,{children:"run specified AI Model as a chatbot"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/serve",children:"ramalama-serve(1)"})}),(0,s.jsx)(a.td,{children:"serve REST API on specified AI Model"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/stop",children:"ramalama-stop(1)"})}),(0,s.jsx)(a.td,{children:"stop named container that is running AI Model"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.a,{href:"/docs/commands/ramalama/version",children:"ramalama-version(1)"})}),(0,s.jsx)(a.td,{children:"display version of RamaLama"})]})]})]}),"\n",(0,s.jsx)(a.h2,{id:"configuration-files",children:"CONFIGURATION FILES"}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"ramalama.conf"})," (",(0,s.jsx)(a.code,{children:"/usr/share/ramalama/ramalama.conf"}),", ",(0,s.jsx)(a.code,{children:"/etc/ramalama/ramalama.conf"}),", ",(0,s.jsx)(a.code,{children:"/etc/ramalama/ramalama.conf.d/*.conf"}),", ",(0,s.jsx)(a.code,{children:"$HOME/.config/ramalama/ramalama.conf"}),", ",(0,s.jsx)(a.code,{children:"$HOME/.config/ramalama/ramalama.conf.d/*.conf"}),")"]}),"\n",(0,s.jsx)(a.p,{children:"RamaLama has builtin defaults for command line options. These defaults can be overridden using the ramalama.conf configuration files."}),"\n",(0,s.jsxs)(a.p,{children:["Distributions ship the ",(0,s.jsx)(a.code,{children:"/usr/share/ramalama/ramalama.conf"})," file with their default settings. Administrators can override fields in this file by creating the ",(0,s.jsx)(a.code,{children:"/etc/ramalama/ramalama.conf"})," file.  Users can further modify defaults by creating the ",(0,s.jsx)(a.code,{children:"$HOME/.config/ramalama/ramalama.conf"})," file. RamaLama merges its builtin defaults with the specified fields from these files, if they exist. Fields specified in the users file override the administrator's file, which overrides the distribution's file, which override the built-in defaults."]}),"\n",(0,s.jsx)(a.p,{children:"RamaLama uses builtin defaults if no ramalama.conf file is found."}),"\n",(0,s.jsxs)(a.p,{children:["If the ",(0,s.jsx)(a.strong,{children:"RAMALAMA_CONFIG"})," environment variable is set, then its value is used for the ramalama.conf file rather than the default."]}),"\n",(0,s.jsx)(a.h2,{id:"environment-variables",children:"ENVIRONMENT VARIABLES"}),"\n",(0,s.jsx)(a.p,{children:"RamaLama default behaviour can also be overridden via environment variables,\nalthough the recommended way is to use the ramalama.conf file."}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"ENV Name"}),(0,s.jsx)(a.th,{children:"Description"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"HTTP_PROXY, http_proxy"}),(0,s.jsx)(a.td,{children:"proxy URL for HTTP connections"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"HTTPS_PROXY, https_proxy"}),(0,s.jsx)(a.td,{children:"proxy URL for HTTPS connections"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"NO_PROXY, no_proxy"}),(0,s.jsx)(a.td,{children:"comma-separated list of hosts to bypass proxy (e.g., localhost,127.0.0.1,.local)"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"RAMALAMA_CONFIG"}),(0,s.jsx)(a.td,{children:"specific configuration file to be used"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"RAMALAMA_CONTAINER_ENGINE"}),(0,s.jsx)(a.td,{children:"container engine (Podman/Docker) to use"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"RAMALAMA_FORCE_EMOJI"}),(0,s.jsxs)(a.td,{children:["define whether ",(0,s.jsx)(a.code,{children:"ramalama run"})," uses EMOJI"]})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"RAMALAMA_IMAGE"}),(0,s.jsx)(a.td,{children:"container image to use for serving AI Model"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"RAMALAMA_IN_CONTAINER"}),(0,s.jsx)(a.td,{children:"Run RamaLama in the default container"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"RAMALAMA_STORE"}),(0,s.jsx)(a.td,{children:"location to store AI Models"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"RAMALAMA_TRANSPORT"}),(0,s.jsx)(a.td,{children:"default AI Model transport (ollama, huggingface, OCI)"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"TMPDIR"}),(0,s.jsx)(a.td,{children:"directory for temporary files. Defaults to /var/tmp if unset."})]})]})]}),"\n",(0,s.jsx)(a.h2,{id:"see-also",children:"See Also"}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.a,{href:"https://github.com/containers/podman/blob/main/docs/source/markdown/podman.1.md",children:"podman(1)"}),", ",(0,s.jsx)(a.strong,{children:"docker(1)"}),", ",(0,s.jsx)(a.a,{href:"/docs/configuration/conf",children:"ramalama.conf(5)"}),", ",(0,s.jsx)(a.a,{href:"/docs/platform-guides/cuda",children:"ramalama-cuda(7)"}),", ",(0,s.jsx)(a.a,{href:"/docs/platform-guides/macos",children:"ramalama-macos(7)"})]}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsxs)(a.em,{children:["Aug 2024, Originally compiled by Dan Walsh <",(0,s.jsx)(a.a,{href:"mailto:dwalsh@redhat.com",children:"dwalsh@redhat.com"}),">"]})})]})}function h(e={}){const{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,a,n)=>{n.d(a,{R:()=>t,x:()=>l});var r=n(6540);const s={},i=r.createContext(s);function t(e){const a=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function l(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(i.Provider,{value:a},e.children)}}}]);