"use strict";(globalThis.webpackChunkdocsite=globalThis.webpackChunkdocsite||[]).push([[132],{6419(e,a,n){n.r(a),n.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"commands/ramalama/convert","title":"convert","description":"convert AI Models from local storage to OCI Image","source":"@site/docs/commands/ramalama/convert.mdx","sourceDirName":"commands/ramalama","slug":"/commands/ramalama/convert","permalink":"/docs/commands/ramalama/convert","draft":false,"unlisted":false,"editUrl":"https://github.com/containers/ramalama/edit/main/docsite/commands/ramalama/convert.mdx","tags":[],"version":"current","frontMatter":{"title":"convert","description":"convert AI Models from local storage to OCI Image"},"sidebar":"docs","previous":{"title":"containers","permalink":"/docs/commands/ramalama/containers"},"next":{"title":"daemon","permalink":"/docs/commands/ramalama/daemon"}}');var r=n(4848),i=n(8453);const s={title:"convert",description:"convert AI Models from local storage to OCI Image"},o="convert",l={},c=[{value:"Synopsis",id:"synopsis",level:2},{value:"Description",id:"description",level:2},{value:"Options",id:"options",level:2},{value:"<strong>--gguf</strong>=<em>Q2_K</em> | <em>Q3_K_S</em> | <em>Q3_K_M</em> | <em>Q3_K_L</em> | <em>Q4_0</em> | <em>Q4_K_S</em> | <em>Q4_K_M</em> | <em>Q5_0</em> | <em>Q5_K_S</em> | <em>Q5_K_M</em> | <em>Q6_K</em> | <em>Q8_0</em>",id:"--ggufq2_k--q3_k_s--q3_k_m--q3_k_l--q4_0--q4_k_s--q4_k_m--q5_0--q5_k_s--q5_k_m--q6_k--q8_0",level:4},{value:"<strong>--help</strong>, <strong>-h</strong>",id:"--help--h",level:4},{value:"<strong>--image</strong>=IMAGE",id:"--imageimage",level:4},{value:"<strong>--network</strong>=<em>none</em>",id:"--networknone",level:4},{value:"<strong>--pull</strong>=<em>policy</em>",id:"--pullpolicy",level:4},{value:"<strong>--rag-image</strong>=IMAGE",id:"--rag-imageimage",level:4},{value:"<strong>--type</strong>=&quot;artifact&quot; | <em>raw</em> | <em>car</em>",id:"--typeartifact--raw--car",level:4},{value:"EXAMPLE",id:"example",level:2},{value:"See Also",id:"see-also",level:2}];function d(e){const a={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h4:"h4",header:"header",hr:"hr",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"convert",children:"convert"})}),"\n",(0,r.jsx)(a.h2,{id:"synopsis",children:"Synopsis"}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"ramalama convert"})," [",(0,r.jsx)(a.em,{children:"options"}),"] ",(0,r.jsx)(a.em,{children:"model"})," [",(0,r.jsx)(a.em,{children:"target"}),"]"]}),"\n",(0,r.jsx)(a.h2,{id:"description",children:"Description"}),"\n",(0,r.jsx)(a.p,{children:"Convert specified AI Model to an OCI Formatted AI Model"}),"\n",(0,r.jsx)(a.p,{children:"The model can be from RamaLama model storage in Huggingface, Ollama, or a local model stored on disk. Converting from an OCI model is not supported."}),"\n",(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsx)(a.p,{children:"The convert command must be run with containers. Use of the --nocontainer option is not allowed."})}),"\n",(0,r.jsx)(a.h2,{id:"options",children:"Options"}),"\n",(0,r.jsxs)(a.h4,{id:"--ggufq2_k--q3_k_s--q3_k_m--q3_k_l--q4_0--q4_k_s--q4_k_m--q5_0--q5_k_s--q5_k_m--q6_k--q8_0",children:[(0,r.jsx)(a.strong,{children:"--gguf"}),"=",(0,r.jsx)(a.em,{children:"Q2_K"})," | ",(0,r.jsx)(a.em,{children:"Q3_K_S"})," | ",(0,r.jsx)(a.em,{children:"Q3_K_M"})," | ",(0,r.jsx)(a.em,{children:"Q3_K_L"})," | ",(0,r.jsx)(a.em,{children:"Q4_0"})," | ",(0,r.jsx)(a.em,{children:"Q4_K_S"})," | ",(0,r.jsx)(a.em,{children:"Q4_K_M"})," | ",(0,r.jsx)(a.em,{children:"Q5_0"})," | ",(0,r.jsx)(a.em,{children:"Q5_K_S"})," | ",(0,r.jsx)(a.em,{children:"Q5_K_M"})," | ",(0,r.jsx)(a.em,{children:"Q6_K"})," | ",(0,r.jsx)(a.em,{children:"Q8_0"})]}),"\n",(0,r.jsxs)(a.p,{children:["Convert Safetensor models into a GGUF with the specified quantization format. To learn more about model quantization, read llama.cpp documentation:\n",(0,r.jsx)(a.a,{href:"https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md",children:"https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md"})]}),"\n",(0,r.jsxs)(a.h4,{id:"--help--h",children:[(0,r.jsx)(a.strong,{children:"--help"}),", ",(0,r.jsx)(a.strong,{children:"-h"})]}),"\n",(0,r.jsx)(a.p,{children:"Print usage message"}),"\n",(0,r.jsxs)(a.h4,{id:"--imageimage",children:[(0,r.jsx)(a.strong,{children:"--image"}),"=IMAGE"]}),"\n",(0,r.jsxs)(a.p,{children:["Image to use for model quantization when converting to GGUF format (when the ",(0,r.jsx)(a.code,{children:"--gguf"})," option has been specified). The image must have the\n",(0,r.jsx)(a.code,{children:"llama-quantize"})," executable available on the ",(0,r.jsx)(a.code,{children:"PATH"}),". Defaults to the appropriate ",(0,r.jsx)(a.code,{children:"ramalama"})," image based on available accelerators. If no\naccelerators are available, the current ",(0,r.jsx)(a.code,{children:"quay.io/ramalama/ramalama"})," image will be used."]}),"\n",(0,r.jsxs)(a.h4,{id:"--networknone",children:[(0,r.jsx)(a.strong,{children:"--network"}),"=",(0,r.jsx)(a.em,{children:"none"})]}),"\n",(0,r.jsx)(a.p,{children:"sets the configuration for network namespaces when handling RUN instructions"}),"\n",(0,r.jsxs)(a.h4,{id:"--pullpolicy",children:[(0,r.jsx)(a.strong,{children:"--pull"}),"=",(0,r.jsx)(a.em,{children:"policy"})]}),"\n",(0,r.jsxs)(a.p,{children:["Pull image policy. The default is ",(0,r.jsx)(a.strong,{children:"missing"}),"."]}),"\n",(0,r.jsxs)(a.h4,{id:"--rag-imageimage",children:[(0,r.jsx)(a.strong,{children:"--rag-image"}),"=IMAGE"]}),"\n",(0,r.jsxs)(a.p,{children:["Image to use when converting to GGUF format (when then ",(0,r.jsx)(a.code,{children:"--gguf"})," option has been specified). The image must have the ",(0,r.jsx)(a.code,{children:"convert_hf_to_gguf.py"})," script\nexecutable and available in the ",(0,r.jsx)(a.code,{children:"PATH"}),". The script is available from the ",(0,r.jsx)(a.code,{children:"llama.cpp"})," GitHub repo. Defaults to the current\n",(0,r.jsx)(a.code,{children:"quay.io/ramalama/ramalama-rag"})," image."]}),"\n",(0,r.jsxs)(a.h4,{id:"--typeartifact--raw--car",children:[(0,r.jsx)(a.strong,{children:"--type"}),'="artifact" | ',(0,r.jsx)(a.em,{children:"raw"})," | ",(0,r.jsx)(a.em,{children:"car"})]}),"\n",(0,r.jsx)(a.p,{children:"Convert the MODEL to the specified OCI Object"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Type"}),(0,r.jsx)(a.th,{children:"Description"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"artifact"}),(0,r.jsx)(a.td,{children:"Store AI Models as artifacts"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"car"}),(0,r.jsx)(a.td,{children:"Traditional OCI image including base image with the model stored in a /models subdir"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"raw"}),(0,r.jsxs)(a.td,{children:["Traditional OCI image including only the model and a link file ",(0,r.jsx)(a.code,{children:"model.file"})," pointed at it stored at /"]})]})]})]}),"\n",(0,r.jsx)(a.h2,{id:"example",children:"EXAMPLE"}),"\n",(0,r.jsx)(a.p,{children:"Generate an oci model out of an Ollama model."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"$ ramalama convert ollama://tinyllama:latest oci://quay.io/rhatdan/tiny:latest\nBuilding quay.io/rhatdan/tiny:latest...\nSTEP 1/2: FROM scratch\nSTEP 2/2: COPY sha256:2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 /model\n--\x3e Using cache 69db4a10191c976d2c3c24da972a2a909adec45135a69dbb9daeaaf2a3a36344\nCOMMIT quay.io/rhatdan/tiny:latest\n--\x3e 69db4a10191c\nSuccessfully tagged quay.io/rhatdan/tiny:latest\n69db4a10191c976d2c3c24da972a2a909adec45135a69dbb9daeaaf2a3a36344\n"})}),"\n",(0,r.jsx)(a.p,{children:"Generate and run an oci model with a quantized GGUF converted from Safetensors."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"$ ramalama convert --gguf Q4_K_M hf://ibm-granite/granite-3.2-2b-instruct oci://quay.io/kugupta/granite-3.2-q4-k-m:latest\nConverting /Users/kugupta/.local/share/ramalama/models/huggingface/ibm-granite/granite-3.2-2b-instruct to quay.io/kugupta/granite-3.2-q4-k-m:latest...\nBuilding quay.io/kugupta/granite-3.2-q4-k-m:latest...\n$ ramalama run oci://quay.io/kugupta/granite-3.2-q4-k-m:latest\n"})}),"\n",(0,r.jsx)(a.h2,{id:"see-also",children:"See Also"}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.a,{href:"/docs/commands/ramalama/",children:"ramalama(1)"}),", ",(0,r.jsx)(a.a,{href:"/docs/commands/ramalama/push",children:"ramalama-push(1)"})]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsxs)(a.em,{children:["Aug 2024, Originally compiled by Eric Curtin <",(0,r.jsx)(a.a,{href:"mailto:ecurtin@redhat.com",children:"ecurtin@redhat.com"}),">"]})})]})}function m(e={}){const{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453(e,a,n){n.d(a,{R:()=>s,x:()=>o});var t=n(6540);const r={},i=t.createContext(r);function s(e){const a=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:a},e.children)}}}]);